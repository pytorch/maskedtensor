{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pytorch/maskedtensor/blob/main/docs/source/notebooks/sparse.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sparsity in PyTorch](https://pytorch.org/docs/stable/sparse.html) is a quickly growing area that has found a lot of support and demand due to its efficiency in both memory and compute. This tutorial is meant to be used in conjunction with the the PyTorch link above, as the sparse tensors are ultimately the building blocks for MaskedTensors (just as regular `torch.Tensor`s are as well).\n",
    "\n",
    "Sparse storage formats have been proven to be powerful in a variety of ways. As a primer, the first use case most practitioners think about is when the majority of elements are equal to zero (a high degree of sparsity), but even in cases of lower sparsity, certain formats (e.g. BSR) can take advantage of substructures within a matrix. There are a number of different [sparse storage formats](https://en.wikipedia.org/wiki/Sparse_matrix) that can be leveraged with various tradeoffs and degrees of adoption.\n",
    "\n",
    "\"Specified\" and \"unspecified\" elements (e.g. elements that are stored vs. not) have a long history in PyTorch without formal semantics and certainly without consistency; indeed, MaskedTensor was partially born out of a build up of issues (e.g. the [nan_grad tutorial](https://pytorch.org/maskedtensor/main/notebooks/nan_grad.html)) that vanilla tensors could not address. A major goal of the MaskedTensor project is to become the primary source of truth for specified/unspecified semantics where they are a first class citizen instead of an afterthought.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note: Currently, only the COO and CSR sparse storage formats are supported in MaskedTensor (BSR and CSC will be developed in the future). If you have another format that you would like supported, please file an issue!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `input` and `mask` must have the same storage format, whether that's `torch.strided`, `torch.sparse_coo`, or `torch.sparse_csr`.\n",
    "\n",
    "2. `input` and `mask` must have the same size, indicated by `t.size()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse COO Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from maskedtensor import masked_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In according with Principle #1, a sparse MaskedTensor is created by passing in two sparse tensors, which can be initialized with any of the constructors, e.g. `torch.sparse_coo_tensor`.\n",
    "\n",
    "As a recap of [sparse COO tensors](https://pytorch.org/docs/stable/sparse.html#sparse-coo-tensors), the COO format stands for \"Coordinate format\", where the specified elements are stored as tuples of their indices and the corresponding values. That is, the following are provided:\n",
    "\n",
    "- `indices`: array of size `(ndim, nse)` and dtype `torch.int64`\n",
    "- `values`: array of size `(nse,)` with any integer or floating point number dtype\n",
    "\n",
    "where `ndim` is the dimensionality of the tensor and `nse` is the number of specified elements\n",
    "\n",
    "By way of example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values:\n",
      " tensor([[0, 0, 3],\n",
      "        [4, 0, 5]])\n",
      "mask:\n",
      " tensor([[False, False,  True],\n",
      "        [False, False,  True]])\n",
      "sparse tensor:\n",
      " tensor([[0, 0, 3],\n",
      "        [4, 0, 5]])\n",
      "masked tensor:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --, 3],\n",
      "    [      --,       --, 5]\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note that the below is the same as:\n",
    "\n",
    "i = [[0, 1, 1],\n",
    "     [2, 0, 2]]\n",
    "v =  [3, 4, 5]\n",
    "m = torch.tensor([True, False, True])\n",
    "\n",
    "values = torch.sparse_coo_tensor(i, v, (2, 3))\n",
    "mask = torch.sparse_coo_tensor(i, m, (2, 3))\n",
    "\n",
    "the following is simply shorter to write / easier to use\n",
    "\"\"\"\n",
    "\n",
    "values = torch.tensor([[0, 0, 3], [4, 0, 5]]).to_sparse()\n",
    "mask = torch.tensor([[False, False, True], [False, False, True]]).to_sparse()\n",
    "\n",
    "mt = masked_tensor(values, mask)  \n",
    "\n",
    "print(\"values:\\n\", values.to_dense())\n",
    "print(\"mask:\\n\", mask.to_dense())\n",
    "print(\"sparse tensor:\\n\", values.to_dense())\n",
    "print(\"masked tensor:\\n\", mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word of warning: when using a function like `.to_sparse_coo()`, if the user does not specify the indices like in the above example, then 0 values will be default \"unspecified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values:\n",
      " tensor(indices=tensor([[0, 1, 1],\n",
      "                       [2, 0, 2]]),\n",
      "       values=tensor([3, 4, 5]),\n",
      "       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n",
      "mask:\n",
      " tensor(indices=tensor([[0, 1],\n",
      "                       [2, 2]]),\n",
      "       values=tensor([True, True]),\n",
      "       size=(2, 3), nnz=2, layout=torch.sparse_coo)\n",
      "mt2:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --, 3],\n",
      "    [      --,       --, 5]\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "i = [[0, 1, 1],\n",
    "     [2, 0, 2]]\n",
    "v =  [3, 4, 5]\n",
    "m = torch.tensor(\n",
    "     [[False, False, True],\n",
    "      [False, False, True]]\n",
    ")\n",
    "\n",
    "values = torch.sparse_coo_tensor(i, v, (2, 3))\n",
    "mask = m.to_sparse_coo()\n",
    "mt2 = masked_tensor(values, mask)\n",
    "\n",
    "print(\"values:\\n\", values)\n",
    "print(\"mask:\\n\", mask)\n",
    "print(\"mt2:\\n\", mt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `mt` and `mt2` will have the same value in the vast majority of operations, but this brings us to a note on the implementation under the hood:\n",
    "\n",
    "`input` and `mask` - only for sparse formats - can have a different number of elements (`tensor.nnz()`) **at creation**, but the indices of `mask` must then be a subset of the indices from `input`. In this case, `input` will assume the shape of mask using the function `input.sparse_mask(mask)`; in other words, any of the elements in `input` that are not `True` in `mask` will be thrown away\n",
    "\n",
    "Therefore, under the hood, the data looks slightly different; `mt` has the 4 value masked out and `mt2` is completely without it. In other words, their underlying data still has different shapes, so `mt + mt2` is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt.masked_data:\n",
      " tensor(indices=tensor([[0, 1],\n",
      "                       [2, 2]]),\n",
      "       values=tensor([3, 5]),\n",
      "       size=(2, 3), nnz=2, layout=torch.sparse_coo)\n",
      "mt2.masked_data:\n",
      " tensor(indices=tensor([[0, 1],\n",
      "                       [2, 2]]),\n",
      "       values=tensor([3, 5]),\n",
      "       size=(2, 3), nnz=2, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "print(\"mt.masked_data:\\n\", mt.masked_data)\n",
    "print(\"mt2.masked_data:\\n\", mt2.masked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse CSR Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, MaskedTensor also supports the [CSR (Compressed Sparse Row)](https://pytorch.org/docs/stable/sparse.html#sparse-csr-tensor) sparse tensor format. Instead of storing the tuples of the indices like sparse COO tensors, sparse CSR tensors aim to decrease the memory requirements by storing compressed row indices. In particular, a CSR sparse tensor consists of three 1-D tensors:\n",
    "\n",
    "- `crow_indices`: array of compressed row indices with size `(size[0] + 1,)`. This array indicates which row a given entry in `values` lives in. The last element is the number of specified elements, while `crow_indices[i+1] - crow_indices[i]` indicates the number of specified elements in row `i`.\n",
    "- `col_indices`: array of size `(nnz,)`. Indicates the column indices for each value.\n",
    "- `values`: array of size `(nnz,)`. Contains the values of the CSR tensor.\n",
    "\n",
    "Of note, sparse CSR tensors are in a [beta](https://pytorch.org/docs/stable/index.html) state, so they are less developed in functionality than that of sparse COO tensors.\n",
    "\n",
    "Again, by way of example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csr tensor:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n",
      "mask csr tensor:\n",
      " tensor([[ True, False],\n",
      "        [False,  True]])\n",
      "masked tensor:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [  1.0000,       --],\n",
      "    [      --,   4.0000]\n",
      "  ]\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61643/2891322065.py:6: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /scratch/georgeqi/work/repos/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:68.)\n",
      "  csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\n"
     ]
    }
   ],
   "source": [
    "crow_indices = torch.tensor([0, 2, 4])\n",
    "col_indices = torch.tensor([0, 1, 0, 1])\n",
    "values = torch.tensor([1, 2, 3, 4])\n",
    "mask_values = torch.tensor([True, False, False, True])\n",
    "\n",
    "csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\n",
    "mask = torch.sparse_csr_tensor(crow_indices, col_indices, mask_values, dtype=torch.bool)\n",
    "\n",
    "mt = masked_tensor(csr, mask)\n",
    "\n",
    "print(\"csr tensor:\\n\", csr.to_dense())\n",
    "print(\"mask csr tensor:\\n\", mask.to_dense())\n",
    "print(\"masked tensor:\\n\", mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[All unary operations are supported](https://pytorch.org/maskedtensor/main/unary.html), e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_tensor(\n",
       "  [\n",
       "    [  0.8415,       --],\n",
       "    [      --,  -0.7568]\n",
       "  ]\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.sin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Binary operations are also supported](https://pytorch.org/maskedtensor/main/binary.html), but the input masks from the two masked tensors must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [[0, 1, 1],\n",
    "     [2, 0, 2]]\n",
    "v1 = [3, 4, 5]\n",
    "v2 = [20, 30, 40]\n",
    "m = torch.tensor([True, False, True])\n",
    "\n",
    "s1 = torch.sparse_coo_tensor(i, v1, (2, 3))\n",
    "s2 = torch.sparse_coo_tensor(i, v2, (2, 3))\n",
    "mask = torch.sparse_coo_tensor(i, m, (2, 3))\n",
    "\n",
    "mt1 = masked_tensor(s1, mask)\n",
    "mt2 = masked_tensor(s2, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt1:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --, 3],\n",
      "    [      --,       --, 5]\n",
      "  ]\n",
      ")\n",
      "mt2:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --, 20],\n",
      "    [      --,       --, 40]\n",
      "  ]\n",
      ")\n",
      "torch.div(mt2, mt1):\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --,   6.6667],\n",
      "    [      --,       --,   8.0000]\n",
      "  ]\n",
      ")\n",
      "torch.mul(mt1, mt2):\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --, 60],\n",
      "    [      --,       --, 200]\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"mt1:\\n\", mt1)\n",
    "print(\"mt2:\\n\", mt2)\n",
    "print(\"torch.div(mt2, mt1):\\n\", torch.div(mt2, mt1))\n",
    "print(\"torch.mul(mt1, mt2):\\n\", torch.mul(mt1, mt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, when the underlying data is sparse, only [reductions](https://pytorch.org/maskedtensor/main/reductions.html) across all dimensions are supported and not a particular dimension (e.g. `mt.sum()` is supported but not `mt.sum(dim=1)`). This is next in line to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [  1.0000,       --],\n",
      "    [      --,   4.0000]\n",
      "  ]\n",
      ")\n",
      "mt.sum():\n",
      " masked_tensor(  5.0000, True)\n",
      "mt.amin():\n",
      " masked_tensor(  1.0000, True)\n"
     ]
    }
   ],
   "source": [
    "print(\"mt:\\n\", mt)\n",
    "print(\"mt.sum():\\n\", mt.sum())\n",
    "print(\"mt.amin():\\n\", mt.amin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaskedTensor methods and sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_dense()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_tensor(\n",
       "  [\n",
       "    [  1.0000,       --],\n",
       "    [      --,   4.0000]\n",
       "  ]\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_sparse_coo()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [[3, 0, 0],\n",
    "     [0, 4, 5]]\n",
    "m = [[True, False, False],\n",
    "     [False, True, True]]\n",
    "mt = masked_tensor(torch.tensor(v), torch.tensor(m))\n",
    "\n",
    "mt_sparse = mt.to_sparse_coo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_sparse_csr()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [[3, 0, 0],\n",
    "     [0, 4, 5]]\n",
    "m = [[True, False, False],\n",
    "     [False, True, True]]\n",
    "mt = masked_tensor(torch.tensor(v), torch.tensor(m))\n",
    "\n",
    "mt_sparse_csr = mt.to_sparse_csr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`is_sparse` / `is_sparse_coo` / `is_sparse_csr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt.is_sparse:  False\n",
      "mt_sparse.is_sparse:  True\n",
      "mt.is_sparse_coo:  False\n",
      "mt_sparse.is_sparse_coo:  True\n",
      "mt.is_sparse_csr:  False\n",
      "mt_sparse_csr.is_sparse_csr:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"mt.is_sparse: \", mt.is_sparse())\n",
    "print(\"mt_sparse.is_sparse: \", mt_sparse.is_sparse())\n",
    "\n",
    "print(\"mt.is_sparse_coo: \", mt.is_sparse_coo())\n",
    "print(\"mt_sparse.is_sparse_coo: \", mt_sparse.is_sparse_coo())\n",
    "\n",
    "print(\"mt.is_sparse_csr: \", mt.is_sparse_csr())\n",
    "print(\"mt_sparse_csr.is_sparse_csr: \", mt_sparse_csr.is_sparse_csr())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f53e922b446dc65d06f6118b3e8c84e0dbdd5cafb12d35760abf4d14ea01879d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
