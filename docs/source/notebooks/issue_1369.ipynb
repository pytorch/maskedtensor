{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8e382d-4bbd-436c-be0c-72b5f12a99ec",
   "metadata": {},
   "source": [
    "# Efficiency of writing \"sparse\" semantics for Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d5bba-65d7-4e92-bcde-327f1ba866b8",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pytorch/maskedtensor/blob/main/docs/source/notebooks/issue_1369.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122c536-7e0a-4a00-9143-e06247db07e3",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398ee60-142c-48b2-ad53-17de3d3cc142",
   "metadata": {},
   "source": [
    "[Issue 1369](https://github.com/pytorch/pytorch/issues/1369) discussed the additional lines of code that were introduce while writing \"sparse\" semantics for Adagrad. But really the code doesn't use sparsity as a compression and optimization technique, it wants to use masked semantics. We worked around this by introducing one-off semantics and operators that encode this behavior while forcing users to be aware of storage details such as indices and values. Let's look at the current implementation of [Adagrad](https://github.com/pytorch/pytorch/blob/master/torch/optim/adagrad.py) [(functional)](https://github.com/pytorch/pytorch/blob/6c2f235d368b697072699e5ca9485fd97d0b9bcc/torch/optim/_functional.py#L16-L51) to illustrate that.\n",
    "\n",
    "In particular we'll point out when sparsity is used as a semantic extension, i.e. unspecified values are not zero and when it is just used to compress zeros. We'll also compare and contrast this with equivalent code written using MaskedTensor. In the end the code snippets are repeat without additional comments to show the difference in brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f673172c-299a-4227-9249-02e234caeebc",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c13dbff-b662-4344-8d5f-e340b27d8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from maskedtensor import masked_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85eba1b",
   "metadata": {},
   "source": [
    "### Original Sparse Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3961042c-b699-4d5f-8ab8-1e162fa3f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param:\n",
      " tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]])\n",
      "grad:\n",
      " tensor([[0., 0., 3., 0.],\n",
      "        [4., 0., 5., 0.]])\n",
      "state_sum:\n",
      " tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "def _make_sparse(grad, grad_indices, values):\n",
    "    size = grad.size()\n",
    "    if grad_indices.numel() == 0 or values.numel() == 0:\n",
    "        return torch.empty_like(grad)\n",
    "    return torch.sparse_coo_tensor(grad_indices, values, size)\n",
    "\n",
    "# We don't support sparse gradients\n",
    "param = torch.arange(8).reshape(2, 4).float()\n",
    "i = torch.tensor([[0, 1, 1],\n",
    "                  [2, 0, 2]])\n",
    "v = torch.tensor([3, 4, 5], dtype=torch.float32)\n",
    "grad = torch.sparse_coo_tensor(i, v, [2, 4])\n",
    "state_sum = torch.full_like(param, 0.5) # initial value for state sum\n",
    "\n",
    "print(\"param:\\n\", param)\n",
    "print(\"grad:\\n\", grad.to_dense())\n",
    "print(\"state_sum:\\n\", state_sum)\n",
    "\n",
    "# Some hyperparameters\n",
    "eps = 1e-10\n",
    "clr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c4cbe4-9a8d-42d3-b0c2-b809aa90b122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000]])\n",
      "state_sum:\n",
      " tensor([[ 0.5000,  0.5000,  9.5000,  0.5000],\n",
      "        [16.5000,  0.5000, 25.5000,  0.5000]])\n",
      "std:\n",
      " tensor([[ 0.0000,  0.0000,  9.5000,  0.0000],\n",
      "        [16.5000,  0.0000, 25.5000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "state_sum = torch.full_like(param, 0.5) # initial value for state sum\n",
    "print(state_sum)\n",
    "\n",
    "grad = grad.coalesce()  # the update is non-linear so indices must be unique\n",
    "grad_indices = grad._indices()\n",
    "grad_values = grad._values()\n",
    "\n",
    "# pow(2) has the same semantics for both sparse and dense memory layouts since 0^2 is zero\n",
    "state_sum.add_(_make_sparse(grad, grad_indices, grad_values.pow(2)))\n",
    "# We take care to make std sparse, even though state_sum clearly is not.\n",
    "# This means that we're only applying the gradient to parts of the state_sum\n",
    "# for which it is specified. This even drives the point home a lot more that\n",
    "# the passed gradient is not sparse, but masked.\n",
    "std = state_sum.sparse_mask(grad)\n",
    "print(\"state_sum:\\n\", state_sum)\n",
    "print(\"std:\\n\", std.to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23a6fc-3c9c-470d-a7cc-8dd7c0148e12",
   "metadata": {},
   "source": [
    "This is where we have a very important divergence. The addition of eps\n",
    "should technically be applied to all values, but instead is only applied to\n",
    "specified values. Here we're using sparsity as a semantic extension and\n",
    "to enforce a certain pattern of defined and undefined values. If parts\n",
    "of the values of the gradient are zero they are still included if materialized.\n",
    "Even though they could be compressed by other sparse storage layouts.\n",
    "This is technically quite brittle even though someone could argue that eps is\n",
    "always very small.\n",
    "\n",
    "More so an implementation add\\_ for sparsity as a storage layout and compression\n",
    "scheme should cause densification, but we force it not to. For this one-off\n",
    "case it is fine until we want to introduce new compression schemes such as\n",
    "CSR, BSR or 2:4 block sparsity. We'll then need to introduce separate Tensor\n",
    "types for each and write variations for gradients compressed using different\n",
    "storage formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f4db2b-a611-4011-85a6-820c180fd96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param:\n",
      " tensor([[0.0000, 1.0000, 1.9027, 3.0000],\n",
      "        [3.9015, 5.0000, 5.9010, 7.0000]])\n"
     ]
    }
   ],
   "source": [
    "# We currently dodge all these concerns using the private method values.\n",
    "std_values = std._values().sqrt_().add_(eps)\n",
    "\n",
    "# We currently don't support div for sparse Tensors because zero / zero is\n",
    "# not well defined. For a MaskedTensor undefined / undefined is undefined.\n",
    "param.add_(_make_sparse(grad, grad_indices, grad_values / std_values), alpha=-clr)\n",
    "print(\"param:\\n\", param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167246d",
   "metadata": {},
   "source": [
    "### MaskedTensor Sparse Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ffeb91-428a-43b1-bb2c-233c17bb3e86",
   "metadata": {},
   "source": [
    "We've been conflating sparsity as an optimization with sparsity as a semantic extension to PyTorch. MaskedTensor proposes to call the semantic extension through sparsity masked. Currently we can't have dense semantics with sparse storage or masked semantics with dense storage. MaskedTensor fixes that because it separates the storage from the semantics. Let's look at above example using a masked gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0dfab3-61c0-453a-a60f-5c784731f344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_grad:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --,   3.0000,       --],\n",
      "    [  4.0000,       --,   5.0000,       --]\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mask = (grad.to_dense() != 0).to_sparse()\n",
    "\n",
    "# Create an entirely new set of parameters to avoid errors\n",
    "param2 = torch.arange(8).reshape(2, 4).float()\n",
    "state_sum2 = masked_tensor(\n",
    "    torch.full_like(param, 0.5).to_sparse(), # initial value for state sum\n",
    "    mask\n",
    ")\n",
    "\n",
    "masked_grad = masked_tensor(grad, mask)\n",
    "print(\"masked_grad:\\n\", masked_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c539bf6-26b6-41a8-8de3-22e1a50a2dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_sum:\n",
      " tensor([[ 0.5000,  0.5000,  9.5000,  0.5000],\n",
      "        [16.5000,  0.5000, 25.5000,  0.5000]])\n",
      "std:\n",
      " tensor(indices=tensor([[0, 1, 1],\n",
      "                       [2, 0, 2]]),\n",
      "       values=tensor([3.0822, 4.0620, 5.0498]),\n",
      "       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n",
      "state_sum2:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --,   0.5000,       --],\n",
      "    [  0.5000,       --,   0.5000,       --]\n",
      "  ]\n",
      ")\n",
      "std2:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --,   9.5000,       --],\n",
      "    [ 16.5000,       --,  25.5000,       --]\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# We can eventually construct a masked std backed by a sparse layout\n",
    "std2 = state_sum2 + masked_grad.pow(2)\n",
    "\n",
    "# Let's print both this version and the regular version for easier comparison\n",
    "print(\"state_sum:\\n\", state_sum)\n",
    "print(\"std:\\n\", std)\n",
    "print(\"state_sum2:\\n\", state_sum2)\n",
    "print(\"std2:\\n\", std2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ebdca4-a791-4d82-bbe9-a005f4d05f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std:\n",
      " tensor(indices=tensor([[0, 1, 1],\n",
      "                       [2, 0, 2]]),\n",
      "       values=tensor([3.0822, 4.0620, 5.0498]),\n",
      "       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n",
      "std2:\n",
      " masked_tensor(\n",
      "  [\n",
      "    [      --,       --,   3.0822,       --],\n",
      "    [  4.0620,       --,   5.0498,       --]\n",
      "  ]\n",
      ")\n",
      "param:\n",
      " tensor([[0.0000, 1.0000, 1.9027, 3.0000],\n",
      "        [3.9015, 5.0000, 5.9010, 7.0000]])\n",
      "param2:\n",
      " tensor([[0.0000, 1.0000, 1.9027, 3.0000],\n",
      "        [3.9015, 5.0000, 5.9010, 7.0000]])\n"
     ]
    }
   ],
   "source": [
    "# We can add support for in-place operations later. Notice how this doesn't\n",
    "# need to access any storage internals and is in general a lot shorter\n",
    "std2 = std2.sqrt().add(eps)\n",
    "\n",
    "print(\"std:\\n\", std)\n",
    "print(\"std2:\\n\", std2)\n",
    "\n",
    "# .data() indeed returns the sparse layout\n",
    "param2 = param2.add((masked_grad / std2).data(), alpha=-clr)\n",
    "\n",
    "# The final results is the same\n",
    "print(\"param:\\n\", param)\n",
    "print(\"param2:\\n\", param2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4f4a6",
   "metadata": {},
   "source": [
    "## Conclusion: Difference in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f7a126",
   "metadata": {},
   "source": [
    "For reference, this is the regular, dense code path without masked gradients or sparsity:\n",
    "\n",
    "```\n",
    "state_sum.addcmul_(grad, grad, value=1)\n",
    "std = state_sum.sqrt().add_(eps)\n",
    "param.addcdiv_(grad, std, value=-clr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278b5f5",
   "metadata": {},
   "source": [
    "The vanilla tensor implementation for sparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "699ed91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 1.8329, 3.0000],\n",
       "        [3.8314, 5.0000, 5.8306, 7.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = grad.coalesce()  # the update is non-linear so indices must be unique\n",
    "grad_indices = grad._indices()\n",
    "grad_values = grad._values()\n",
    "size = grad.size()\n",
    "\n",
    "state_sum.add_(_make_sparse(grad, grad_indices, grad_values.pow(2)))\n",
    "std = state_sum.sparse_mask(grad)\n",
    "std_values = std._values().sqrt_().add_(eps)\n",
    "param.add_(_make_sparse(grad, grad_indices, grad_values / std_values), alpha=-clr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05c49a",
   "metadata": {},
   "source": [
    "While MaskedTensor minimizes the code to the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f33bd5a-f867-41e9-971b-286d10825790",
   "metadata": {},
   "outputs": [],
   "source": [
    "std2 = state_sum2 + masked_grad.pow(2)\n",
    "std2 = std2.sqrt().add(eps)\n",
    "param2 = param2.add((masked_grad / std2).data(), alpha=-clr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2f3e5",
   "metadata": {},
   "source": [
    "And for good measure, let's make sure the parameters match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d1272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param:\n",
      " tensor([[0.0000, 1.0000, 1.9027, 3.0000],\n",
      "        [3.9015, 5.0000, 5.9010, 7.0000]])\n",
      "param2:\n",
      " tensor([[0.0000, 1.0000, 1.8053, 3.0000],\n",
      "        [3.8031, 5.0000, 5.8020, 7.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(\"param:\\n\", param)\n",
    "print(\"param2:\\n\", param2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f53e922b446dc65d06f6118b3e8c84e0dbdd5cafb12d35760abf4d14ea01879d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
